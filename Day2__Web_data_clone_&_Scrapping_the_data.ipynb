{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsB2ELlYrKMxE4dHHO2MOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daggubati9/Daggubati9.github.io/blob/main/Day2__Web_data_clone_%26_Scrapping_the_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cuU4J80sH9y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eMCq0UJrfkML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Square(num):\n",
        "    return num**2\n",
        "print(Square(3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dibfbAE4sPNf",
        "outputId": "5ce65ac2-dd42-475c-c35f-249d979e0b7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Send a GET request to the website\n",
        "url = 'https://www.prashantisarees.com/en-us/collections/kanjivaram-silk-sarees'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 2: Parse the page content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 3: Find and print the data (example: all article titles in <h2> tags)\n",
        "for title in soup.find_all('h2'):\n",
        "    print(title.text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1yai2-9i0XY",
        "outputId": "44779475-4228-44c8-a893-6224f08130e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kanchipuram Silk Sarees\n",
            "1309 products\n",
            "Login\n",
            "Shop the look\n",
            "Get new arrivals in your inbox!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Send a GET request to the website\n",
        "url = 'https://www.bluecrossma.org/?gad_source=1&gbraid=0AAAAADzX1uzDbji6e5GtHYcXjBYdE2LzM&gclid=Cj0KCQjwoNzABhDbARIsALfY8VOzZOqWZJUs4skV7C2obJ3DzKyDztqxAqVh3QbxJTKxtps53_EqPHoaAs-7EALw_wcB&gclsrc=aw.dsg'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 2: Parse the page content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 3: Find and print the data (example: all article titles in <h2> tags)\n",
        "for title in soup.find_all('h2'):\n",
        "    print(title.text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R80YaUdzjM_n",
        "outputId": "60e0429f-fce5-49fa-e077-ab50b727b528"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other Sites\n",
            "Need Help\n",
            "EASILY ACCESS YOUR PLAN WITH MYBLUE\n",
            "A more personalized approach to finding care\n",
            "Medicare plans that fit you\n",
            "The care you need. Whenever and wherever.\n",
            "Recognized for Health Equity by NCQA\n",
            "we put the blue in bluebikes\n",
            "Hear from our CEO\n",
            "Footer: Links\n",
            "Download App\n",
            "Follow us:\n",
            "Choose a language:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: URL to scrape\n",
        "url = 'https://www.bluecrossma.org/?gad_source=1&gbraid=0AAAAADzX1uzDbji6e5GtHYcXjBYdE2LzM&gclid=Cj0KCQjwoNzABhDbARIsALfY8VOzZOqWZJUs4skV7C2obJ3DzKyDztqxAqVh3QbxJTKxtps53_EqPHoaAs-7EALw_wcB&gclsrc=aw.ds'\n",
        "\n",
        "# Step 2: Send a GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 3: Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 4: Extract visible text from <p>, <h1>-<h3>, etc.\n",
        "content = []\n",
        "\n",
        "# Add the title of the page\n",
        "page_title = soup.title.string if soup.title else 'No Title Found'\n",
        "content.append(f\"Page Title: {page_title}\\n\")\n",
        "\n",
        "# Extract paragraphs\n",
        "for para in soup.find_all('p'):\n",
        "    text = para.get_text(strip=True)\n",
        "    if text:\n",
        "        content.append(text)\n",
        "\n",
        "# Extract headings\n",
        "for tag in ['h1', 'h2', 'h3']:\n",
        "    for heading in soup.find_all(tag):\n",
        "        text = heading.get_text(strip=True)\n",
        "        if text:\n",
        "            content.append(f\"{tag.upper()}: {text}\")\n",
        "\n",
        "# Step 5: Save content to a text file\n",
        "with open(\"scraped_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for line in content:\n",
        "        file.write(line + \"\\n\")\n",
        "\n",
        "print(\"✅ Content downloaded and saved to 'scraped_content.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkk4C_xaky79",
        "outputId": "ee8411e0-44de-45f0-b26b-bdf0e6c2bedd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Content downloaded and saved to 'scraped_content.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given data\n",
        "total_employees = 400\n",
        "pay_rise = 312\n",
        "pension_benefits = 248\n",
        "both = 173\n",
        "neither = 13\n",
        "\n",
        "# a) Getting a pay rise\n",
        "prob_pay_rise = pay_rise / total_employees\n",
        "\n",
        "# b) Not getting a pay rise\n",
        "prob_no_pay_rise = (total_employees - pay_rise) / total_employees\n",
        "\n",
        "# c) Getting both a pay rise and pension benefits\n",
        "prob_both = both / total_employees\n",
        "\n",
        "# d) Getting no pay rise or benefit increase (neither)\n",
        "prob_neither = neither / total_employees\n",
        "\n",
        "# e) Getting a pay rise or benefits (union)\n",
        "# Using the formula: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
        "prob_union = (pay_rise + pension_benefits - both) / total_employees\n",
        "\n",
        "# Print results\n",
        "print(f\"a) Probability of getting a pay rise: {prob_pay_rise:.3f}\")\n",
        "print(f\"b) Probability of not getting a pay rise: {prob_no_pay_rise:.3f}\")\n",
        "print(f\"c) Probability of getting both a pay rise and pension benefits: {prob_both:.3f}\")\n",
        "print(f\"d) Probability of getting no pay rise or benefit increase: {prob_neither:.3f}\")\n",
        "print(f\"e) Probability of getting a pay rise or benefits: {prob_union:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dk8uo8xtvAf",
        "outputId": "a7a4d80f-924c-445c-8409-4eb3ea8d542c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) Probability of getting a pay rise: 0.780\n",
            "b) Probability of not getting a pay rise: 0.220\n",
            "c) Probability of getting both a pay rise and pension benefits: 0.432\n",
            "d) Probability of getting no pay rise or benefit increase: 0.033\n",
            "e) Probability of getting a pay rise or benefits: 0.968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlRlFfqNNsKO",
        "outputId": "efcf7bd8-83e7-4908-ca36-4eb320b1b657"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2025.4.26)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.1.0 webdriver_manager-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "import tempfile\n",
        "\n",
        "# Setup Chrome options with a temporary profile to avoid session error\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")  # avoids profile conflict\n",
        "chrome_options.add_argument(\"--headless\")  # run in background\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Set path to chromedriver if not in PATH\n",
        "# Example: Service(\"/path/to/chromedriver\")\n",
        "service = Service()\n",
        "\n",
        "# Initialize the WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://www.indeed.com/jobs?q=data+scientist&l=Bolton%2C+MA&from=searchOnHP%2Cwhatautocomplete%2Cwhereautocomplete&vjk=2c15f6f156f07bdb\"\n",
        "query = \"data+scientist\"\n",
        "location = \"Bolton%2C+MA\"\n",
        "\n",
        "# List to store job data\n",
        "job_listings = []\n",
        "\n",
        "# Scrape first 5 pages (10 jobs per page)\n",
        "for page in range(0, 50, 10):\n",
        "    url = f\"{base_url}?q={query}&l={location}&start={page}\"\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    job_cards = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
        "\n",
        "    for card in job_cards:\n",
        "        try:\n",
        "            title = card.find_element(By.CLASS_NAME, 'jobTitle').text\n",
        "        except NoSuchElementException:\n",
        "            title = ''\n",
        "        try:\n",
        "            company = card.find_element(By.CLASS_NAME, 'companyName').text\n",
        "        except NoSuchElementException:\n",
        "            company = ''\n",
        "        try:\n",
        "            location = card.find_element(By.CLASS_NAME, 'companyLocation').text\n",
        "        except NoSuchElementException:\n",
        "            location = ''\n",
        "        try:\n",
        "            summary = card.find_element(By.CLASS_NAME, 'job-snippet').text\n",
        "        except NoSuchElementException:\n",
        "            summary = ''\n",
        "\n",
        "        job_listings.append({\n",
        "            'Title': title,\n",
        "            'Company': company,\n",
        "            'Location': location,\n",
        "            'Summary': summary\n",
        "        })\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(job_listings)\n",
        "df.to_csv('indeed_jobs.csv', index=False)\n",
        "print(\"Scraping complete. Saved to 'indeed_jobs.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ3x-L1zO1zO",
        "outputId": "e9c82d3a-9896-4fe9-a2e5-0b2f64f8a449"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Saved to 'indeed_jobs.csv'.\n"
          ]
        }
      ]
    }
  ]
}